{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for n00bs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop 3: Python for Data Science\n",
    "Welcome to the last workshop of the series! My name is Vikram Mark Radhakrishnan. You can find me on [LinkedIn](https://www.linkedin.com/in/vikram-mark-radhakrishnan-90038660/), or reach me via email at radhakrishnan@strw.leidenuniv.nl\n",
    "\n",
    "Shout-out to the [AI Lab One](https://www.meetup.com/AI-Lab/) and [City AI](https://city.ai/) for making this workshop possible!\n",
    "<img src=\"nb_images/AI_Lab.png\">  \n",
    "In today's workshop, we are going to take a look at several different libraries written in Python, that are requisite for every data scientist. These libraries facilitate cleaning and pre-processing of data, making exploratory data analysis and visualizations, building simple machine learning models and training, validating, and testing them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. NumPy - building a strong foundation\n",
    "The [NumPy](https://www.numpy.org/) library is ubiquitous in data science, as well as a whole bunch of other fields. It enables powerful computing with N-dimensional arrays, and faster calculations due to storage efficient data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with creating a bunch of numpy objects. These objects are called arrays, and can have 0 or more dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = np.array([42]) # A zero-dimensional array is called a scalar\n",
    "vector = np.array([4, 8, 15, 16, 23, 42]) # A one-dimensional array is called a vector\n",
    "matrix = np.array([[2, 7, 6], [9, 5, 1], [4, 3, 8]]) # A two-dimensional array is called a matrix\n",
    "tensor = np.array([[[8, 24, 10], [12, 7, 23], [22, 11, 9]],\n",
    "                   [[15, 1, 26], [25, 14, 3], [2, 27, 13]],\n",
    "                   [[19, 17, 6], [5, 21, 16], [18, 4, 20]]]) # An array with higher than 3 dimensions is a tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful to know: You can use the Python operator @ to take the dot product of two matrices, or you can use np.dot(M1, M2). To take the elementwise product, use np.multiply. And to take the transpose of a matrix you can use M.T or np.transpose(M)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToDo:** Write a function to check if the transpose of a numpy matrix is equal to its inverse, i.e. A.A^t = I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More useful numpy methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones_matrix = np.ones([2,2])\n",
    "zeros_matrix = np.zeros([2,2])\n",
    "empty_matrix = np.empty([2,2])\n",
    "full_matrix = np.full((2,2), 7)\n",
    "identity_matrix = np.eye(2)\n",
    "random_matrix = np.random.random((2,2))\n",
    "\n",
    "range_of_values = np.arange(1.3, 4.7, 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indexing numpy arrays is identical to indexing Python lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Visualizing data with Matplotlib\n",
    "The [matplotlib.pyplot](https://matplotlib.org) library is one of the most widely used plotting and visulization tools used with Python. You can plot figures and graphs that look very similar to the ones generated with MATLAB code with this library, and which are aesthetically appealing and comprehensible.\n",
    "\n",
    "The code in this section can also be found in this [official tutorial](https://matplotlib.org/users/pyplot_tutorial.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at some simple 2d plots with plt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just a line through some numbers\n",
    "plt.plot([1,2,3,4])\n",
    "plt.ylabel('some numbers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The x and y values of some coordinates, denoted with red dots\n",
    "plt.plot([1,2,3,4], [1,4,9,16], 'ro')\n",
    "plt.axis([0, 6, 0, 20])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evenly sampled time at 200ms intervals\n",
    "t = np.arange(0., 5., 0.2)\n",
    "\n",
    "# red dashes, blue squares and green triangles\n",
    "plt.plot(t, t, 'r--', t, t**2, 'bs', t, t**3, 'g^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the width of the line\n",
    "x = np.array([1,2,3,4])\n",
    "y = np.array([1,4,9,16])\n",
    "plt.plot(x, y, lw=2.0)\n",
    "plt.plot(x, y**2, c='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When making multiple plots in the same figure, we need to use the plt.subplot feature. The parameter passed to plt.subplot is three digits. The first digit is the total number of subplots needed. The second digit is the number of rows of subplots, and the third digit is the number of columns of subplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a figure with multiple axes\n",
    "def f(t):\n",
    "    return np.exp(-t) * np.cos(2*np.pi*t)\n",
    "\n",
    "t1 = np.arange(0.0, 5.0, 0.1)\n",
    "t2 = np.arange(0.0, 5.0, 0.02)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.subplot(211)\n",
    "plt.plot(t1, f(t1), 'bo', t2, f(t2), 'k')\n",
    "\n",
    "plt.subplot(212)\n",
    "plt.plot(t2, np.cos(2*np.pi*t2), 'r--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make multiple figures with multiple axes\n",
    "plt.figure(1)                # the first figure\n",
    "plt.subplot(211)             # the first subplot in the first figure\n",
    "plt.plot([1, 2, 3])\n",
    "plt.subplot(212)             # the second subplot in the first figure\n",
    "plt.plot([4, 5, 6])\n",
    "\n",
    "\n",
    "plt.figure(2)                # a second figure\n",
    "plt.plot([4, 5, 6])          # creates a subplot(111) by default\n",
    "\n",
    "plt.figure(1)                # figure 1 current; subplot(212) still current\n",
    "plt.subplot(211)             # make subplot(211) in figure1 current\n",
    "plt.title('Easy as 1, 2, 3') # subplot 211 title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clear the current figure using plt.clf() and the current axis using plt.cla()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding text to the plots\n",
    "\n",
    "np.random.seed(19680801)\n",
    "\n",
    "mu, sigma = 100, 15\n",
    "x = mu + sigma * np.random.randn(10000)\n",
    "\n",
    "# the histogram of the data\n",
    "n, bins, patches = plt.hist(x, 50, density=1, facecolor='g', alpha=0.75)\n",
    "\n",
    "\n",
    "plt.xlabel('Smarts')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Histogram of IQ')\n",
    "plt.text(60, .025, r'$\\mu=100,\\ \\sigma=15$')\n",
    "plt.axis([40, 160, 0, 0.03])\n",
    "plt.grid(True)\n",
    "plt.title(r'$\\sigma_i=15$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's how we annotate with text\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "t = np.arange(0.0, 5.0, 0.01)\n",
    "s = np.cos(2*np.pi*t)\n",
    "line, = plt.plot(t, s, lw=2)\n",
    "\n",
    "plt.annotate('local max', xy=(2, 1), xytext=(3, 1.5),\n",
    "            arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "            )\n",
    "\n",
    "plt.ylim(-2,2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change the scaling of the axes using plt.xscale and plt.yscale. You can choose among linear, logarithmic, or logit scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToDo:** Make a list of peoples heights and weights and plot them out, and label the points by the persons' names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "heights = []\n",
    "weights = []\n",
    "\n",
    "plt.plot()\n",
    "for person in range(len(names)):\n",
    "    plt.text()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tabulating data with pandas\n",
    "The [pandas](https://pandas.pydata.org) library provides powerful data structures for efficient handling of tabulated or labeled data. It has built in methods to deal with missing data, to differentiate between numeric data, categorical data, and date/timestamps.\n",
    "\n",
    "The code in this section can also be found in this [tutorial](https://pandas.pydata.org/pandas-docs/stable/getting_started/10min.html#min)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a series of data and store it as a pandas dataframe\n",
    "s = pd.Series([1, 3, 5, np.nan, 6, 8])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe using a numpy array, with a datetime index and labeled columns\n",
    "dates = pd.date_range('20130101', periods=6)\n",
    "df = pd.DataFrame(np.random.randn(6, 4), index=dates, columns=list('ABCD'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or use a dict of objects that can be converted into a series\n",
    "df2 = pd.DataFrame({'A': 1.,\n",
    "                    'B': pd.Timestamp('20130102'),\n",
    "                    'C': pd.Series(1, index=list(range(4)), dtype='float32'),\n",
    "                    'D': np.array([3] * 4, dtype='int32'),\n",
    "                    'E': pd.Categorical([\"test\", \"train\", \"test\", \"train\"]),\n",
    "                    'F': 'foo'})\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The head and tail methods are useful for viewing small, relevant subsets of your dataframe.  \n",
    "While the index and columns methods are useful for separating the actual data (columns) from their timestamps or dates or index.  \n",
    "The describe method shows a statistical summary of your data.  \n",
    "Or you can use descriptive statistics such as df.mean() or df.median() for a column by column calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the to_numpy() function to convert a pandas dataframe into a numpy 2-dimensional array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can sort your data by a particular axis, or by a particular column's values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by='B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas dataframes are sort of similar to Python dictionaries, in the way that you can select and refer to data. Here are some ways of selecting data from a pd dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For getting a cross section using a label\n",
    "df.loc[dates[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting on a multi-axis by label:\n",
    "df.loc[:, ['A', 'B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label slicing\n",
    "df.loc['20130102':'20130104', ['A', 'B']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting values from a dataframe by their positions\n",
    "df.iloc[3:5, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or by explicitly mentioning indexes\n",
    "df.iloc[[1, 2, 4], [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or using Boolean conditions to slice/subset a dataframe\n",
    "df[df.A > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting anew column\n",
    "E = np.arange(6)\n",
    "df['E'] = E\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas offers many elegant ways to deal with missing data. You can drop missing data using df.dropna(how=...) or you can fill in missing data with df.fillna(value=...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series([1, 3, 5, np.nan, 6, 8], index=dates).shift(2)\n",
    "df.sub(s, axis='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Breaking a dataframe into pieces and merging it with concat\n",
    "df = pd.DataFrame(np.random.randn(10, 4))\n",
    "pieces = [df[:3], df[3:7], df[7:]]\n",
    "pd.concat(pieces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appending data to a dataframe\n",
    "df = pd.DataFrame(np.random.randn(8, 4), columns=['A', 'B', 'C', 'D'])\n",
    "s = df.iloc[3]\n",
    "df.append(s, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at grouping data. This is a blanket term to cover\n",
    "* Splitting the data into groups based on some criteria\n",
    "* Applying a function to each group independently\n",
    "* Combining the results into a data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'A': ['foo', 'bar', 'foo', 'bar',\n",
    "                         'foo', 'bar', 'foo', 'foo'],\n",
    "                   'B': ['one', 'one', 'two', 'three',\n",
    "                         'two', 'two', 'one', 'three'],\n",
    "                   'C': np.random.randn(8),\n",
    "                   'D': np.random.randn(8)})\n",
    "\n",
    "df.groupby(['A', 'B']).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas provides ways to efficiently handle categorical data. Let's take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"id\": [1, 2, 3, 4, 5, 6],\n",
    "                   \"raw_grade\": ['a', 'b', 'b', 'a', 'a', 'e']})\n",
    "df[\"grade\"] = df[\"raw_grade\"].astype(\"category\")\n",
    "df[\"grade\"].cat.categories = [\"very good\", \"good\", \"very bad\"]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"grade\"] = df[\"grade\"].cat.set_categories([\"very bad\", \"bad\", \"medium\", \"good\", \"very good\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save dataframes in csv files using the df.to_csv(...) method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can read csv files using URLs with pandas. For example, let's take a look at some [Housing data from Amsterdam](data.amsterdam.nl) with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.data.amsterdam.nl/dataselectie/bag/export/?openbare_ruimte=Kalverstraat'\n",
    "df1 = pd.read_csv(url, sep=\";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ToDo**: Experiment with your own datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Learning from data with scikit-learn\n",
    "The [scikit-learn](https://scikit-learn.org) library is a standard machine learning resource for a Python programming data scientist. It includes methods to partition data into training, test, and validation sets, and several machine learning models.\n",
    "\n",
    "The code in this section can also be found in this [tutorial](https://www.datacamp.com/community/tutorials/machine-learning-python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand sklearn, we will go through an example of loading a dataset, doing some exploratory analysis on it, and then training a machine learning model on it to function as a predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first task is to load the dataset. We will use one of the example datasets available to us in the scikit-learn library, called \"digits\". The digits dataset is a collection of handwritten digits, each image being 8 pixels by 8 pixels. When loaded as a dataset, each row of the dataset represents one image, and each of the 64 columns is the value of one pixel in this image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "#Load the digits dataset\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "#Display the first digit\n",
    "plt.figure(1, figsize=(3, 3))\n",
    "plt.imshow(digits.images[-1], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do some preliminary exploratory analysis on our data. This will give us an idea of what our data looks like and how we should go about building a classifier with this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the keys of the `digits` data\n",
    "print(digits.keys())\n",
    "\n",
    "# Print out the data\n",
    "print(digits.data)\n",
    "\n",
    "# Print out the target values\n",
    "print(digits.target)\n",
    "\n",
    "# Print out the description of the `digits` data\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look a bit more into the content of our dataset. The shapes of our training data and labels (targets in this case) tell us how much data we have to work with, and what its dimensions are, and how many labels or categories it falls into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate the `digits` data\n",
    "digits_data = digits.data\n",
    "\n",
    "# Inspect the shape\n",
    "print(digits_data.shape)\n",
    "\n",
    "# Isolate the target values with `target`\n",
    "digits_target = digits.target\n",
    "\n",
    "# Inspect the shape\n",
    "print(digits_target.shape)\n",
    "\n",
    "# Print the number of unique labels\n",
    "number_digits = len(np.unique(digits.target))\n",
    "\n",
    "# Isolate the `images`\n",
    "digits_images = digits.images\n",
    "\n",
    "# Inspect the shape\n",
    "print(digits_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there is no better way to visualize data than by plotting it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size (width, height) in inches\n",
    "fig = plt.figure(figsize=(6, 6))\n",
    "\n",
    "# Adjust the subplots \n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "# For each of the 64 images\n",
    "for i in range(64):\n",
    "    # Initialize the subplots: add a subplot in the grid of 8 by 8, at the i+1-th position\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    # Display an image at the i-th position\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are working with data that has several features. These features could be amount of light/dark pixels, number of straight lines, number of curved lines, number of loops, etc. However, working with so many different features is difficult for amachine learning algorithm, and often unnecessary. This is because certain features do not contribute much more information than other features, and hence these features can be combined into a single feature to reduce dimensionality. Principal Component Analysis or PCA attempts to do just this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a regular PCA model \n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the data to the model\n",
    "reduced_data_pca = pca.fit_transform(digits.data)\n",
    "\n",
    "# Inspect the shape\n",
    "reduced_data_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['black', 'blue', 'purple', 'yellow', 'white', 'red', 'lime', 'cyan', 'orange', 'gray']\n",
    "for i in range(len(colors)):\n",
    "    x = reduced_data_pca[:, 0][digits.target == i]\n",
    "    y = reduced_data_pca[:, 1][digits.target == i]\n",
    "    plt.scatter(x, y, c=colors[i])\n",
    "plt.legend(digits.target_names, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "plt.xlabel('First Principal Component')\n",
    "plt.ylabel('Second Principal Component')\n",
    "plt.title(\"PCA Scatter Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we work with our data in a machine learning model, we need to pre-process it first. Raw data is almost never ready to be used by a mchine learning algorithm, and data preprocessing is usually the longest and most intensive part of a data scientists job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "\n",
    "# Apply `scale()` to the `digits` data\n",
    "# This gives the data a mean of 0 and a variance of 1\n",
    "data = scale(digits.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we split our data into a training set and a testing set. With very large datasets it is also useful to have a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `train_test_split`\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the `digits` data into training and test sets\n",
    "X_train, X_test, y_train, y_test, images_train, images_test = train_test_split(data, digits.target, digits.images, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the numbers after this split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of training features\n",
    "n_samples, n_features = X_train.shape\n",
    "\n",
    "# Print out `n_samples`\n",
    "print(n_samples)\n",
    "\n",
    "# Print out `n_features`\n",
    "print(n_features)\n",
    "\n",
    "# Number of Training labels\n",
    "n_digits = len(np.unique(y_train))\n",
    "\n",
    "# Inspect `y_train`\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use a clustering algorithm to build our classifier. We choose a clustering algorithm in this case because it makes the most sense given the kind of data we have - digits with certain features being more similar than others, hence more closely \"clustered\" in feature-space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `cluster` module\n",
    "from sklearn import cluster\n",
    "\n",
    "# Create the KMeans model\n",
    "clf = cluster.KMeans(init='k-means++', n_clusters=10, random_state=42)\n",
    "\n",
    "# Fit the training data `X_train`to the model\n",
    "clf.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the cluster centers by plotting them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure size in inches\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "\n",
    "# Add title\n",
    "fig.suptitle('Cluster Center Images', fontsize=14, fontweight='bold')\n",
    "\n",
    "# For all labels (0-9)\n",
    "for i in range(10):\n",
    "    # Initialize subplots in a grid of 2X5, at i+1th position\n",
    "    ax = fig.add_subplot(2, 5, 1 + i)\n",
    "    # Display images\n",
    "    ax.imshow(clf.cluster_centers_[i].reshape((8, 8)), cmap=plt.cm.binary)\n",
    "    # Don't show the axes\n",
    "    plt.axis('off')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we have a trained model! Let's try to visualize how well our classifier has fitted to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and fit the `digits` data to the PCA model\n",
    "X_pca = PCA(n_components=2).fit_transform(X_train)\n",
    "\n",
    "# Compute cluster centers and predict cluster index for each sample\n",
    "clusters = clf.fit_predict(X_train)\n",
    "\n",
    "# Create a plot with subplots in a grid of 1X2\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Adjust layout\n",
    "fig.suptitle('Predicted Versus Training Labels', fontsize=14, fontweight='bold')\n",
    "fig.subplots_adjust(top=0.85)\n",
    "\n",
    "# Add scatterplots to the subplots \n",
    "ax[0].scatter(X_pca[:, 0], X_pca[:, 1], c=clusters)\n",
    "ax[0].set_title('Predicted Training Labels')\n",
    "ax[1].scatter(X_pca[:, 0], X_pca[:, 1], c=y_train)\n",
    "ax[1].set_title('Actual Training Labels')\n",
    "\n",
    "# Show the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use our model to predict on the test set, and see how well we do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import `metrics` from `sklearn`\n",
    "from sklearn import metrics\n",
    "\n",
    "# Predict the labels for `X_test`\n",
    "y_pred=clf.predict(X_test)\n",
    "\n",
    "# Print out the confusion matrix with `confusion_matrix()`\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not going to lie, this is pretty pathetic! Okay let's try to use a different machine learning algorithm on this same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the `svm` model\n",
    "from sklearn import svm\n",
    "\n",
    "# Create the SVC model \n",
    "svc_model = svm.SVC(gamma=0.001, C=100., kernel='linear')\n",
    "\n",
    "# Fit the data to the SVC model\n",
    "svc_model.fit(X_train, y_train)\n",
    "\n",
    "svc_model.score(X_test, y_test)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is incredible! We have almost 100% accuract using the Support Vector Machine model! Let's visualize this in a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the predicted values to `predicted`\n",
    "predicted = svc_model.predict(X_test)\n",
    "\n",
    "# Zip together the `images_test` and `predicted` values in `images_and_predictions`\n",
    "images_and_predictions = list(zip(images_test, predicted))\n",
    "\n",
    "# For the first 4 elements in `images_and_predictions`\n",
    "for index, (image, prediction) in enumerate(images_and_predictions[:4]):\n",
    "    # Initialize subplots in a grid of 1 by 4 at positions i+1\n",
    "    plt.subplot(1, 4, index + 1)\n",
    "    # Don't show axes\n",
    "    plt.axis('off')\n",
    "    # Display images in all subplots in the grid\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    # Add a title to the plot\n",
    "    plt.title('Predicted: ' + str(prediction))\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Some more useful libraries and your next steps!\n",
    "This list of libraries is by no means extensive, but it is enough to get you start on your journey to being a data scientist. Other useful libraries to learn would be:\n",
    "1. [sqlite3](https://docs.python.org/2/library/sqlite3.html): A library that let's you use SQL commands within Python to construct and query databases.\n",
    "2. [ggplot](http://ggplot.yhathq.com/): A plotting and visualization library that R programmers are very familiar with.\n",
    "3. [seaborn](https://seaborn.pydata.org/): A very high level plotting interface based on matplotlib, that allows you to make fancy and interactive graphics.\n",
    "4. [TensorFlow](https://www.tensorflow.org): A deep learning library developed by Google. Complex syntax, but is useful for making machine learning models that can be deployed on various platforms and devices.\n",
    "5. [PyTorch](https://pytorch.org): A deep learning library developed by Facebook. Easy syntax that is similar to numpy.\n",
    "6. [Keras](https://keras.io/): A high level deep learning library built on top of, and within TensorFlow.\n",
    "7. [FastAI](https://fast.ai): A high level deep learning library built on top of PyTorch, which is the best resource for winnig competitions on Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have now come to the end of your three week journey with Python. You have access to the three Python notebooks we worked with, and you can also use these resources to further your knowledge of Python programming and become more proficient.  \n",
    "1. [Python for Everybody Specialization](https://www.coursera.org/learn/python): A series of online courses offered by the University of Michigan, which starts from the absolute basics of programming, and takes you to a high level of proficiency. It is comprehensive, yet easy to follow.\n",
    "2. [Datacamp](https://www.datacamp.com/): A website with multiple online courses on various topics, including Python programming, fundamentals of data science, and machine learning.\n",
    "3. [Kaggle](https://kaggle.com): A website where you can participate in several data science competitions, and improve your skills.\n",
    "\n",
    "Thank you for being a part of these workshops! I wish you luck in your future endeavours."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
